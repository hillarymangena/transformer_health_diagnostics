Transformer Health and Diagnostics Application: Technical OverviewOverviewThe Transformer Health and Diagnostics application is a cutting-edge, AI-driven solution designed to monitor and maintain the health of electrical transformers in real-time. Built using Python and Flask, this application leverages local large language models (LLMs) and advanced data processing techniques to provide actionable insights, predictive maintenance plans, and interactive diagnostics. Developed on Kali Linux with Python 3.13, it integrates seamlessly with local LLMs (e.g., llama3.2:latest and mxbai-embed-large:latest) via Ollama, demonstrating robust offline AI capabilities and scalability for industrial use cases.Key FeaturesReal-Time Monitoring:Continuously simulates and tracks key transformer parameters (current, voltage, temperature, vibrations, DGA for TX2, and moisture) for three transformers (TX1, TX2, TX3) located in Midrand, Birch Acres, and Kempton West.
Utilizes Chart.js for dynamic, interactive graphs, updated every 30 seconds with simulated data stored in an SQLite database.
Implements alert generation based on predefined normal ranges, with color-coded statuses (green, yellow, red) to indicate health conditions.

Smart Diagnostics with Copilot:Features an AI-powered chat interface using Retrieval-Augmented Generation (RAG) with a Chroma vector store and OllamaLLM.
Allows users to query transformer diagnostics (e.g., "When was TX1 last maintained?") with responses generated by the local LLM, showcasing natural language processing (NLP) and contextual reasoning.
Supports file uploads (e.g., maintenance logs) for RAG enhancement, processed via LangChain's text splitting and embedding.

Maintenance Planning:Generates detailed maintenance plans for each transformer based on the last 24 hours of sensor data or all available data.
Determines whether "Fixing Issues" or "Predictive Maintenance" is required, including issues/faults, possible causes, fixes, and required tools, all explained with transparent AI reasoning.
Offers downloadable plans in text format, timestamped for traceability (e.g., "11:01 AM SAST, October 15, 2025").

Prompt Engineering:Provides a dedicated interface for users to customize LLM prompts, saved to a local file for future use, enabling adaptive AI behavior tailored to specific needs.

Key PagesDashboard (/):The main entry point displaying real-time graphs and an alerts section.
Passes alerts_data to the template, rendering transformer-specific alerts (e.g., "TX1 - Midrand" with status updates) and graph data for visualization.
Includes navigation to other sections via a responsive menu.

Smart Diagnostics (/smart-diagnostics):Hosts the Copilot chat window and Maintenance Planning section.
The chat interface interacts with the /chat endpoint, while the "Generate Maintenance Plan" button triggers /generate_maintenance_plan.

Prompt Engineering (/prompt_engineering):A form-based page for submitting custom prompts, processed via a POST request to save them locally.

API Endpoint (/api/alerts):A RESTful endpoint returning JSON with transformer data, alerts, and graph data, polled by the frontend for real-time updates.

Main Programming Concepts AppliedWeb Framework (Flask):Utilizes Flask for a lightweight, modular server setup with RESTful routing and template rendering (Jinja2).
Implements debug mode for rapid development and error tracking.

Database Management (SQLite):Employs SQLite for persistent storage of sensor data and uploaded files, with CRUD operations handled via Python's sqlite3 module.
Ensures data integrity with transaction commits and error handling.

AI and Machine Learning (LangChain, Ollama):Leverages LangChain for RAG, integrating OllamaLLM and OllamaEmbeddings for local LLM inference without API keys.
Uses langchain-chroma for vector storage, enabling efficient semantic search and context-aware responses.
Applies prompt engineering to guide LLM outputs, with timeout handling (30 seconds) for robustness.

Asynchronous Data Simulation:Implements a threaded periodic simulation (every 30 seconds) to mimic real-time sensor data, stored in SQLite for historical analysis.
Replaces WebSocket with REST for reliability, aligning with the original design.

Frontend Integration (HTML, CSS, JavaScript):Employs Chart.js for data visualization and vanilla JavaScript for DOM manipulation and AJAX calls.
Uses a responsive CSS design with a hamburger menu for mobile compatibility.

Error Handling and Logging:Incorporates try-except blocks across LLM initialization, database operations, and file I/O to ensure graceful failure.
Logs errors to the console for debugging, enhancing maintainability.

Pertinent InformationDependencies: Requires flask, langchain, langchain-community, langgraph, langchain-ollama, langchain-chroma, python-dotenv, and websocket-client. Install with pip install flask langchain langchain-community langgraph langchain-ollama langchain-chroma python-dotenv websocket-client.
Setup:Run python3 -m venv venv and source venv/bin/activate.
Start Ollama locally with docker run -d -p 11434:11434 --name llama3.2 a80c4f17acd5.
Execute python app.py to launch the server on http://127.0.0.1:5000.

Compatibility: Optimized for Python 3.11â€“3.13; Python 3.13 may require adjustments due to library support.
Security: Currently in development mode; production deployment requires a WSGI server (e.g., Gunicorn) and authentication for /prompt_engineering.
Scalability: Designed for offline use with local LLMs, extensible to multiple transformers or cloud deployment with API keys.
Version: As of October 15, 2025, this is a prototype with ongoing enhancements planned.

